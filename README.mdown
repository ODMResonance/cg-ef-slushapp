# Search Application for CG Expert Finder

This is the front-end web application that provides search and data interrogation functionality for the **CG Expert Finder** system. This is the web application that would be used by the business to search across all of the data that's been ingested by the [CG Expert Finder Data Hub project](https://github.com/MJMoody/cg-ef-datahub).

This README assumes you have already installed and configured the CG Expert Finder Data Hub project, and that you have ingested and harmonized the data into the FINAL database.

This search application was generated with the [Slush-MarkLogic-Node](https://github.com/marklogic/slush-marklogic-node)
generator, and uses the following components:

- [AngularJS](https://angularjs.org/)
- [Gulp](http://gulpjs.com/)
- [node.js](http://nodejs.org/): very thin layer, hosting the Angular code and proxying MarkLogic REST API requests
- [Roxy Deployer](https://github.com/marklogic/roxy): bootstrap MarkLogic databases, application servers, etc; scaffolding for MarkLogic REST API service extensions


## Required Dependencies

To deploy and run this application you will need:

- [node.js v0.10+](http://nodejs.org/download/)
- [npm v2+](https://www.npmjs.com/): Built-in package manager for node (comes with node)
- [gulp](http://gulpjs.com/): Javascript task automation (`npm install -g gulp`)
- [Bower](http://bower.io/): A package manager for front-end libraries (`npm install -g bower`)
- [Git](https://git-scm.com/) - Roxy depends on this version control system
- [Ruby v1.9.3+](https://www.ruby-lang.org/en/documentation/installation/) - Roxy depends on Ruby in order to run server configuration scripts

Note: the `node` command has been renamed to `nodejs`. Some dependencies still point to `node`, which is out of our control.
You may need to manually alias those commands, or install the `node-legacy` package.

Microsoft Windows users should also read the [Additional Microsoft Windows Requirements](WINDOWS.mdown).


## Instructions

**NOTE ON FOLDER STRUCTURE:** Again, it is assumed here that you have already installed the CG Expert Finder Data Hub project. If you want to mirror the directory structure of the original CG Expert Finder project, you will want to use the base directory that you created to house the CG EF Data Hub project. For example, you should have something like...

    /Users/mmoody/Projects/cg-expertfinder
        /cg-ef-datahub

And through the steps below, you will create another project folder here for the Slush application.

**...**

Navigate to your base folder (i.e., `/cg-expertfinder`) and clone this repo using the following command.

    git clone https://github.com/MJMoody/cg-ef-datahub.git cg-ef-slushapp

This will copy the contents of this code repository into a new folder called `cg-ef-slushapp`. So again, assuming you are mirroring the original project structure, you should now have...

    /Users/mmoody/Projects/cg-experfinder
        /cg-ef-datahub
        /cg-ef-slushapp

In your terminal, navigate to your new `/cg-ef-slushapp` directory.






## Contents

This `cg-ef-datahub` directory will contain two directories: `/input` and `/plugins`.

The `/plugins` folder contains the Ingest and Harmonise routines that we will use within the Data Hub.

The `/input` folder contains subfolders for each source data file that will be ingested using Data Hub. Also contained here, you will find a number of `_BLANK.csv` files -- these are placeholders for the actual source data files that you will be ingesting. Replace the `_BLANK.csv` files with the actual source CSV files.

**IMPORTANT:** You must ensure that your actual source CSV files have the same structure as the `_BLANK.csv` files provided.

## Instructions

**NOTE ON FOLDER STRUCTURE:** If you want to mirror the directory structure of the original CG Expert Finder project, first setup a new base directory (ie `/cg-expertfinder`) that will contain both the Data Hub project and the Slush (frontend UI) project. For example, you would end up with something like...

    /Users/jsmith/Projects/cg-expertfinder
        /cg-ef-datahub
        /cg-ef-slushapp
**...**

Unzip or clone this **cg-ef-datahub** repo onto your local machine. You will now have a `/cg-ef-datahub` folder with the contents described above.

Replace the `_BLANK.csv` files with your actual source CSV files, as described above.

Download the latest [v1 Quick Start application](https://github.com/marklogic-community/marklogic-data-hub/releases) from Releases section of the [MarkLogic Data Hub repository](https://github.com/marklogic-community/marklogic-data-hub). Place the Quick Start `.war` file into the same `/cg-ef-datahub` folder.

Start up the Quick Start (Java) application using the `java -jar quick-start-1****.war` command (replace **** with your actual file name).

Open a browser and run the Quick Start application. This will step you through configuring a new Data Hub environment...

- First, click Browse and select your `cg-ef-datahub` directory as the Project Folder.
- Quick Start will then want to initialise the project
- Provide a DataHub Name -- you can use the default `data-hub` or something more specific `cg-ef-datahub` -- this will be used as the naming prefix for all of the MarkLogic databases created for the Data Hub (ie, cg-ef-datahub-STAGING, cg-ef-datahub-FINAL, etc.)
- Provide the Host location of where you are running MarkLogic -- for a local MarkLogic install, simply use `localhost`
- Under Advanced Settings, you will see the config options for each MarkLogic database that will be created by the Data Hub -- you'll want to ensure that the Port numbers specified are available on your MarkLogic server -- you can leave all the other Advanced Settings unchanged
- Click the Intialize button
- You can select the default `local` Project Environment, and then click Next >
- Login to the Data Hub using your MarkLogic server credentials
- Quick Start will then ask to install the Data Hub Framework. Click INSTALL -- this will take a moment to complete
- Once the install completes, you can go to the Entities tab of the Quick Start application, and you should be able to see the expected Entities, Input and Harmonize Flows.

At this stage, you are ready to ingest the source data into the MarkLogic Data Hub Framework. Assuming you have your source data CSV files in the correct `/input` folder locations, you can then run the Input Flows to bring the source data into the STAGING database, and then run the "Employee" Harmonize Flow to merge and move data into the FINAL database.

## Next Steps

Setup the frontend search application using [MarkLogic's Slush generator](https://github.com/marklogic-community/slush-marklogic-node).










## Quick Start (for the Impatient)

On Mac or Linux:

    npm install
    bower install
    gulp init-local
    ./ml local install
    ./ml local mlcp -options_file import-sample-data.options

On Windows:

    npm install
    bower install
    gulp init-local
    ml.bat local install
    ml.bat local mlcp -options_file import-sample-data.options

## Prepare your Application

Go to the generated application directory:

    cd <app-name>

The generator installs NodeJS and JavaScript dependencies automatically,
but if shared among colleagues, they likely need to run the following commands
manually:

    npm install
    bower install

The generator also creates a `local.json` and a `deploy/local.properties`, but
those are usually gitignored. Check if they exist. If they do, check the settings,
most importantly the port settings to make sure they list available ports. If
those files don't exist, then use the following command to create them:

    gulp init-local

Note: you can use `gulp init-dev` and `gulp init-prod` to setup properties for
the dev and prod environments.

Note: the deploy properties allow tweaking the MarkLogic side of the application
in great detail. Look inside `deploy/build.properties` or the documentation of
the [Roxy deployer](https://github.com/marklogic/roxy) to learn more about this.

Note: consider editing the `_loginMode` variable in `ui/app/login/login.service.js`
to change the login mode of the application. The default is set to `full`. The
other options are `top-right` or `modal`.

## Deploy your Application

Run the following Roxy commands to deploy the application to the chosen MarkLogic
installation. It will create and configure databases, REST servers, users, and
roles for you, and deploy the back-end application code.

    ./ml local bootstrap
    ./ml local deploy modules
    ./ml local deploy content

Or on Windows:

    ml.bat local bootstrap
    ml.bat local deploy modules
    ml.bat local deploy content

## Launch your Application

To launch the middle-tier, and open a browser tab automatically, you only need to run:

    gulp serve-local

Note: this will also watch the `ui/` folder for changes, compile them, and reload the
browser fully automatically.

Note: to get more debug logging from Express, run with:

    DEBUG=http,mail,express:* gulp serve-local

Note: it is possible to override settings on the command-line:

    gulp serve-local --ml-host=<your-host> --ml-http-port=8234 --app-port=9234 --nosync

Note: `--nosync` will cause no extra browsersync reload proxy to get launched at 3000,
and no browser tab will be opened automatically. In addition you have `--verbose`, and
`--ignoreErrors` (for `gulp build`). A few more flags are documented at the top of
`gulpfile.js`.

## Loading Sample Data (optional)

The application comes with 3000 JSON documents generated by json-generator.com. They
will allow you to explore all the features you get out of the box in a better way. You
can load them with (MLCP)[https://docs.marklogic.com/guide/ingestion/content-pump] using
Roxy.

Before you hit off, check if Roxy can find MLCP:

    ./ml local mlcp

Roxy will print the following message if it cannot find MLCP:

    ERROR: MLCP not found or mis-configured, please check the mlcp-home setting.

Roxy looks for `/usr/local/mlcp/` (or `\usr\local\mlcp\`) by default. It can be
convenient to just install MLCP there, or create a symlink from `/usr/local/mlcp`
to where MLCP is installed. Alternatively, edit your `deploy/local.properties`, and
append a `mlcp-home=/path/to/your/mlcp/` to the end of it. Repeating above command
should show MLCP usage, not an ERROR.

Once MLCP works correctly, you can run:

    ./ml local mlcp -options_file import-sample-data.options

Or on Windows:

    ml.bat local mlcp -options_file import-sample-data.options

Note: the detail controller, the part that handles showing your data, can not only
handle JSON, but also XML, Binary, and Text data out of the box.

## Deployment on Server

At some point you might want to deploy and run this as a service on some
(demo) server. For details on that see [INSTALL.mdown](INSTALL.mdown).

## Next Steps

Documentation about the slush generator is provided on the [Generator Wiki](https://github.com/marklogic/slush-marklogic-node/wiki). It covers amongst others:

- [Project folder structure](https://github.com/marklogic/slush-marklogic-node/wiki/Project-folder-structure), which describes the project directory as produces by the generator.

- [Explaining the stack](https://github.com/marklogic/slush-marklogic-node/wiki/Explaining-the-stack), which gives a brief description of the overall architecture.

- [Core Tools and Components](https://github.com/marklogic/slush-marklogic-node/wiki/Core-Tools-and-Components), which allows you to learn more about the various tools and components used by or for the application.

- [Production Deployments](https://github.com/marklogic/slush-marklogic-node/wiki/Run-In-Production), which helps you understand best practices when deploying to a production environment

Last but not least, the landing page of the out of the box application provides many more links.
